{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPOdKTG0US9t"
   },
   "source": [
    "The objective of this experiment is to understand word2vec, by seeing it in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDTJ3UXdUS9v"
   },
   "source": [
    "In this experiment we will use **Mahabharata** as our text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q5bGBzVwUS9w"
   },
   "source": [
    "#### Keywords\n",
    "\n",
    "* Word2Vec\n",
    "* Representation\n",
    "* Stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Renbl5wUS97"
   },
   "source": [
    "The problem with count-based representations is that \n",
    "\n",
    "1.  they are costly in terms of memory\n",
    "\n",
    "2. they discard all context and meaning of words\n",
    "\n",
    "\n",
    "A better way to do this is by using a representation called \"Word2Vec\" with transforms each word into 300-dimensional vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l72Cp7USUS98"
   },
   "source": [
    "#### Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJUrWMcNUS9-"
   },
   "outputs": [],
   "source": [
    "#vector space modeling and topic modeling toolkit\n",
    "import gensim\n",
    "\n",
    "# Operating System\n",
    "import os\n",
    "\n",
    "# Regular Expression\n",
    "import re\n",
    "\n",
    "# nltk packages\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Basic Packages\n",
    "import numpy as np, pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WI9RkeXzUS-C"
   },
   "source": [
    "**Snowball** is a small string processing language designed for creating stemming algorithms for use in Information Retrieval. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_GfauhuUS-D"
   },
   "source": [
    "#### Creating a new instance of a language specific subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hb3haIdJUS-E"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "#very similar to Porter stemmer. A little faster though. And a little more aggresive while stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsQRpmkIUS-I"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "1. Cleaning dataset for text encoding issues :- Very useful when dealing with non-unicode characters. Most often when you read files prepared on Windows, in a Linux/Unix machine\n",
    "2. Creating a set of vocabulary excluding the stopwords\n",
    "3. Stemming a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "ran\n",
      "run\n",
      "author\n",
      "author\n",
      "author\n",
      "author\n",
      "author\n",
      "matric\n",
      "matrix\n",
      "polic\n",
      "polici\n",
      "european\n",
      "europ\n",
      "stock\n",
      "stock\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"running\"))\n",
    "print(stemmer.stem(\"ran\"))\n",
    "print(stemmer.stem(\"runs\"))\n",
    "\n",
    "print(stemmer.stem(\"authorize\"))\n",
    "print(stemmer.stem(\"authorized\"))\n",
    "print(stemmer.stem(\"authority\"))\n",
    "print(stemmer.stem(\"authorization\"))\n",
    "print(stemmer.stem(\"authorizing\"))\n",
    "\n",
    "print(stemmer.stem(\"matrices\"))\n",
    "print(stemmer.stem(\"matrix\"))\n",
    "print(stemmer.stem(\"police\"))\n",
    "print(stemmer.stem(\"policy\"))\n",
    "print(stemmer.stem(\"european\"))\n",
    "print(stemmer.stem(\"europe\"))\n",
    "print(stemmer.stem(\"stocking\"))\n",
    "print(stemmer.stem(\"stocks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lTPQhdT3gLq4"
   },
   "outputs": [],
   "source": [
    "stopWords = pd.read_csv('stopwords.txt').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OBaRsHK0gJWN"
   },
   "outputs": [],
   "source": [
    "class Load_Data(object):\n",
    "    def __init__(self, fnamelist):\n",
    "        self.fnamelist = fnamelist\n",
    "        # Creating a set of vocabulary\n",
    "        self.vocabulary = set([])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in self.fnamelist:\n",
    "            for line in open(fname, encoding='latin1'):\n",
    "                words = re.findall(r'(\\b[a-z][a-z]*\\b)', line.lower())\n",
    "                words = [word for word in words if not word in stopWords]\n",
    "                for word in words:\n",
    "                    self.vocabulary.add(word)\n",
    "                yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qL6Wat1hgPlW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 35s, sys: 3.83 s, total: 7min 39s\n",
      "Wall time: 7min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MB_txt = Load_Data(['MB.txt'])\n",
    "model = gensim.models.Word2Vec(MB_txt, min_count=100)\n",
    "#min_count (int, optional) â€“ Ignores all words with total frequency lower than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "?gensim.models.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GwHJw19gR50"
   },
   "outputs": [],
   "source": [
    "model.save(\"MB2Vec_Without_stemmer.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxUtZNYZ7Ovb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kesava similarity: 0.87\n",
      "Name: vasudeva similarity: 0.78\n",
      "Name: govinda similarity: 0.76\n",
      "Name: madhava similarity: 0.75\n",
      "Name: arjuna similarity: 0.71\n",
      "\n",
      "\n",
      "Checking if similarity is symmetric\n",
      "Name: partha similarity: 0.89\n",
      "Name: dhananjaya similarity: 0.85\n",
      "Name: kama similarity: 0.84\n",
      "Name: bhima similarity: 0.81\n",
      "Name: vrikodara similarity: 0.79\n"
     ]
    }
   ],
   "source": [
    "krishna5_without_stemmer = model.wv.most_similar('krishna')[:5]\n",
    "\n",
    "for name, similarity in krishna5_without_stemmer:\n",
    "    print(\"Name: {} similarity: {}\".format(name, round(similarity,2)))\n",
    "\n",
    "print(\"\\n\\nChecking if similarity is symmetric\")\n",
    "krishna5_without_stemmer = model.wv.most_similar('arjuna')[:5]\n",
    "\n",
    "for name, similarity in krishna5_without_stemmer:\n",
    "    print(\"Name: {} similarity: {}\".format(name, round(similarity,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xERgMkdVUS-J"
   },
   "outputs": [],
   "source": [
    "class Load_Data_stemmed(object):\n",
    "    def __init__(self, fnamelist):\n",
    "        self.fnamelist = fnamelist\n",
    "        # Creating a set of vocabulary\n",
    "        self.vocabulary = set([])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in self.fnamelist:\n",
    "            for line in open(fname, encoding='latin1'):\n",
    "                words = re.findall(r'(\\b[a-z][a-z]*\\b)', line.lower())\n",
    "                # Stemming a word.\n",
    "                words = [ stemmer.stem(word) for word in words if not word in stopWords]\n",
    "                for word in words:\n",
    "                    self.vocabulary.add(word)\n",
    "                yield words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtdzGuOHUS-O"
   },
   "source": [
    "Now, Let us read the data using an iterator in the class defined above, which is a memory-friendly iterator. Save the pretrained vectors using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vp9vy2jqUS-Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 47s, sys: 1.5 s, total: 11min 49s\n",
      "Wall time: 11min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MB_txt_stemmed = Load_Data_stemmed(['MB.txt'])\n",
    "model = gensim.models.Word2Vec(MB_txt_stemmed, min_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRh2HKenevQl"
   },
   "outputs": [],
   "source": [
    "model.save(\"MB2Vec_With_stemmer.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qsLVCmquUS-W"
   },
   "source": [
    "Now Let us see what are the similar words related to certain characters names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gqx5BtMhUS-X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kesava similarity: 0.83\n",
      "Name: madhava similarity: 0.77\n",
      "Name: vasudeva similarity: 0.76\n",
      "Name: govinda similarity: 0.76\n",
      "Name: arjuna similarity: 0.72\n"
     ]
    }
   ],
   "source": [
    "krishna5_with_stemmer =  model.wv.most_similar('krishna')[:5]\n",
    "for name, similarity in krishna5_with_stemmer:\n",
    "  print(\"Name: {} similarity: {}\".format(name, round(similarity,2)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MB2VEC1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
